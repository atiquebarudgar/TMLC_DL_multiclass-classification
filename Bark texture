import numpy as np
import pandas as pd 
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing.image import load_img
from keras.utils import to_categorical 
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import random
import os



filenames = os.listdir('C:/Users/LENOVO/Documents/Edvancer_DL_Coding/Project_TMLC/Bark/DATA/')

categories = []
for filename in filenames:
    category = filename.split(' ')[0]
    if category == 'Acacia': 
        categories.append(0)  #The categories are set as labels
    elif category == 'Barringtonia': 
        categories.append(1)
    elif category == 'Cedrus':
        categories.append(2)
    elif category == 'Citrus':
        categories.append(3)
        
df = pd.DataFrame({
    'filename': filenames,  
    'category': categories,    
})

df

from keras.models import Sequential #Different model are Sequential, functional API
from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization

model2 = Sequential()

# Adding convo layer, 32-features to be extracted from image feeded in model
# 3X3- convo window which will iterate; input_shape of image= WXH-150X150; [3-rgb value, 1-mono colour scale]
# Convolution layer will iterate over image, and will return convolued feature matrix.
model2.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150,150, 3))) 
# Adding max pooling layer- 2x2-size of max pooling window, which will iterate over convolued feature matrix,
# It will downside it, and retain the convolued feature.
# Size will always be smaller than convo size, and 3x3 convo matrix will be converted to now 2x2 size.
model2.add(MaxPooling2D(pool_size=(2, 2)))
# Dropout layer will mainly reduces the complexity of neural network(NN). It reduces the no. of nodes in NN.
model2.add(Dropout(0.25))

# Adding another convo layer with more no of features to be extracted (64).
model2.add(Conv2D(64, (3, 3), activation='relu'))
model2.add(MaxPooling2D(pool_size=(2, 2)))
model2.add(Dropout(0.25))

# Flatten the images 2D tensor into 1D tensor.
model2.add(Flatten())
# Dense layer represents the nodes in the fully connected layer.
model2.add(Dense(96, activation='relu')) # 96=32+64, adding all previous convo layer.
# Adding another dropout layer
model2.add(Dropout(0.5))
# Adding final dense layer- which is equal to number of outputs which we are going to predict.
model2.add(Dense(4, activation='softmax')) # 4 because we have four categories of bark



# Call back is a function of Keras
# Early stopping will stop our model when it is not improving.
# learning_rate_reduction- it will stop earning rate to improve on the matrix which we are specifying, 
# i.e. validation loss not improving, so that it will reduce in next epoch

from keras.callbacks import EarlyStopping, ReduceLROnPlateau

earlystop = EarlyStopping(patience=10)
learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', 
                                            patience=5, 
                                            verbose=1, 
                                            factor=0.5, 
                                            min_lr=0.00001) 
callbacks = [earlystop, learning_rate_reduction]

df["category"] = df["category"].replace({ 0: 'Acacia', 1: 'Barringtonia', 2: 'Cedrus', 3: 'Citrus' })

# Now we will compile our mode
# loss = categorical instead of binary since it is multi class classification
# Optimizer= Adam or SGD or..
model2.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# now we are going to summarize our model.
model2.summary()

# Total params- no. features the NN will extract
# Output Shape can be calculated using formulae


# Here both data is in same folder otherwise you can create (and extract) from two different folder for train and validation data images.
# using train test split for train and validation data, because data is in same folder.
train_df, validate_df = train_test_split(df, test_size=0.20, random_state=0) # random_state=0 to prevent random splitting.
# Here train and validation files got split and images are different from each other.


# Lets see total 'TRAIN' files and in each category
train_df = train_df.reset_index(drop=True)

train_df['category'].value_counts().plot.bar()
print(train_df['category'].value_counts())
print("\n Total train images: ", train_df.shape[0])


total_train = train_df.shape[0]
print("Total train images:", total_train)

total_validate = validate_df.shape[0] 
print("Total validate images:", total_validate)

# data augmentation will increase our data 3 or 5 times, it will make data larger
# We will do augmentation on train data only.
train_datagen = ImageDataGenerator(
                                    rotation_range=15, # it will rotate image in right and left direction.
                                    rescale=1./255, # if you don not resace, NN will not be able to learn
                                    shear_range=0.1, # It will change image of features, by slanting it little.
                                    zoom_range=0.2, # Zoom in
                                    horizontal_flip=True, # Allow images to flip over
                                    width_shift_range=0.1, # shift_range- we will shift image by up, down etc by the given value
                                    height_shift_range=0.1
)

validation_datagen = ImageDataGenerator(rescale=1./255) # No augmentation on validation data only skewing.

train_generator = train_datagen.flow_from_dataframe(
    train_df, 
    "C:/Users/LENOVO/Documents/Edvancer_DL_Coding/Project_TMLC/Bark/DATA/", 
    x_col='filename', # important to give x and y column, on thisit will learn on.
    y_col='category',
    target_size=(150,150), # target image size.
    class_mode='categorical',
    batch_size=32 # no. of images you want to train in each batch, i.e from 33rd image second batch will go.
)

validation_generator = validation_datagen.flow_from_dataframe(
    validate_df, 
    "C:/Users/LENOVO/Documents/Edvancer_DL_Coding/Project_TMLC/Bark/DATA/", 
    x_col='filename',
    y_col='category',
    target_size=(150,150),
    class_mode='categorical',
    batch_size=32
)

epochs=50
batch_size=50 # larger batch size will give you shorter training time.

history = model2.fit_generator(
    train_generator, 
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=total_validate/batch_size,
    steps_per_epoch=total_train/batch_size,
    callbacks=callbacks
)

# If model stopped before completing epoch numbers, there is early stopping means model performnace doesnt improved.
# Lower validation loss, means model is doin good.
# If validation loss is higher than training loss, it is overfitting, then we will use dropout layer to reduce overfitting.


# Plots of training and validataion loss and accuracy.

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6,6))
ax1.plot(history.history['loss'], color='b', label="Training loss")
ax1.plot(history.history['val_loss'], color='r', label="validation loss")
ax1.set_xticks(np.arange(1, epochs, 1))
ax1.set_yticks(np.arange(0, 1, 0.1))
ax1.legend()

ax2.plot(history.history['acc'], color='b', label="Training accuracy")
ax2.plot(history.history['val_acc'], color='r',label="Validation accuracy")
ax2.set_xticks(np.arange(1, epochs, 1))
ax2.legend()

legend = plt.legend(loc='best', shadow=True)
plt.tight_layout()
plt.show()

model2.save('barktexture.hdf5')  # save our model, to avoid re-run for test.

from keras import models    
model = models.load_model('barktexture.hdf5', compile = False)

test_filenames = os.listdir("C:/Users/LENOVO/Documents/Edvancer_DL_Coding/Project_TMLC/Bark/TEST")
test_df = pd.DataFrame({
    'filename': test_filenames
})
total_test = test_df.shape[0]  
print("Total amount of test images are:", total_test)

test_datagen = ImageDataGenerator(rescale=1./255) 
batch_size = 15

test_generator = test_datagen.flow_from_dataframe(
    test_df, 
    "C:/Users/LENOVO/Documents/Edvancer_DL_Coding/Project_TMLC/Bark/TEST",
    x_col='filename',
    y_col=None, # Since we dont have categories in test data, we will set it none.
    class_mode=None,
    target_size=(150,150),
    batch_size=batch_size,
    shuffle=False 
)


# predict on test geneartor
# steps= no. of steps; np.ceil will do roundoff.
predict = model.predict_generator(test_generator, steps=np.ceil(total_test/batch_size)) # generator will iterate over each image.

# Get total predicteed images       

print("Number of predicted images:", len(predict), "\n")

# Get predictions in array
# These are labels 0, 1, 2 (category)
# Use [?:?] to specify range of index in list/array

print("    0:Acacia        1:Barringtonia       2:Cedrus   3:Citrus \n\n", predict, "\n")   


# Argmax= Convert the values o array into integer.
test_df['category'] = np.argmax(predict, axis=-1) 

# Lets see the some predicted values for each category of images
# Use [?:?] to specify range 

print(test_df['category'][0:4]) 


# In mapping we will get strings instead of integers.


print(train_generator.class_indices) 
print(validation_generator.class_indices, "\n")

# Map labels 
label_map = dict((v,k) for k,v in train_generator.class_indices.items())
test_df['category'] = test_df['category'].replace(label_map)
print(label_map, "\n")

# See the output of mapped labels
print(test_df['category'][0:4], "\n")

# Lets replace letters with words to see better
print(test_df['category'][0:4])


test_df['category'].value_counts()
print(test_df['category'].value_counts())

# View the above in a bar graph
test_df['category'].value_counts().plot.bar()

# Lets print the total predicted images 
print("\nTotal predicted images:", test_df.shape[0])


sample_test = test_df.head(18)
sample_test.head()
plt.figure(figsize=(12, 24))
for index, row in sample_test.iterrows():
    filename = row['filename']
    category = row['category']
    img = load_img("C:/Users/LENOVO/Documents/Edvancer_DL_Coding/Project_TMLC/Bark/TEST/" +filename, target_size=(150,150)) #rescale(1./255)
    plt.subplot(9, 4, index+1)      
    plt.imshow(img)
    plt.xlabel(filename + "\n Predicted is " + "{}".format(category))
plt.tight_layout()
plt.show()


submission_df = test_df.copy()
submission_df['id'] = submission_df['filename'].str.split('.').str[0] # id= filename
submission_df['label'] = submission_df['category']
submission_df.drop(['filename', 'category'], axis=1, inplace=True)
submission_df.to_csv('Catdogcar1.csv', index=False)

submission_df.head(300)
